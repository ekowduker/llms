{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd7996c5-91eb-41de-ac6a-0b22462b76f1",
   "metadata": {},
   "source": [
    "# Retrieving Legal Information\n",
    "This note book employs a large language model to retrieve legal information from case files in pdf format. All documents are in the public domain.  \n",
    "  \n",
    "Case: A South African case involving the Road Accident Fund  \n",
    "Retrieval method:  Auto-merging Retrieval  \n",
    "\n",
    "Auto merging is a technique used to optimize the structure and performance of an index. It works by periodically merging smaller segments of the index into larger, more efficient ones. This process can improve search performance by:\n",
    "\n",
    "- Reducing Overhead: Smaller index segments can create overhead due to the need to manage and search through many separate pieces. Merging them reduces this overhead.\n",
    "- Improving Cache Efficiency: Larger, merged segments can be more cache-friendly, making searches faster.\n",
    "- Enhancing Compression: Merging can also improve compression, reducing the storage space required and potentially improving I/O performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b4338",
   "metadata": {},
   "source": [
    "#### Set up the environment  \n",
    "Note: The glob module provides a function for making file lists from directory wildcard searches. Essentially, it allows you to find all the pathnames matching a specified pattern according to the rules used by the Unix shell, although the results are returned in arbitrary order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9218cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\r\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "# This notebook runs on python 3.10.13 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8360b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the required modules\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.node_parser import HierarchicalNodeParser\n",
    "from llama_index.node_parser import get_leaf_nodes\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.retrievers import AutoMergingRetriever\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import Document\n",
    "\n",
    "from trulens_eval import Tru\n",
    "\n",
    "import utils # Ensure utils.py is in the current working directory\n",
    "from utils import get_prebuilt_trulens_recorder\n",
    "import glob\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = utils.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573df55",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load and preprocess the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475e160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_directory_path=\"./ocr_docs_georg/*\" #This is the path to the folder containing the documents\n",
    "\n",
    "# Put all files in the directory into input_files\n",
    "input_files = glob.glob(my_directory_path)\n",
    "\n",
    "# Load the documents\n",
    "documents = SimpleDirectoryReader(input_files=input_files).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ad555b-48d4-441b-b5f3-79a685b4c3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "4 \n",
      "\n",
      "<class 'llama_index.schema.Document'>\n",
      "Doc ID: f2fce791-a18d-4b74-b34e-c89b10a02d29\n",
      "Text: COURT ONLINE COVER PAGE INI THE HIGH COURT OF SOUTH. AFRICA\n",
      "Gauteng Local Division, Pretoria CASE NO: 2023-134420 Int the matter\n",
      "between: THE ROAD ACCIDENT FUND Panuf/Aplon/Agpelant and THE LEGAL\n",
      "PRACTICE COUNCIL,THE BOARD OF SHERIFFS,THE SHERIFF PRETORIA\n",
      "CENTRAL,THE SHERIFF PRETORIA EAST,THE SHERIFF CENTURION EAST,THE\n",
      "SHERIFF OHANNESBURG CENTRA...\n"
     ]
    }
   ],
   "source": [
    "# Check document contents\n",
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47510e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all documents into one\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc863b8b",
   "metadata": {},
   "source": [
    "#### Set up the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b466a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm_name=\"gpt-3.5-turbo\"\n",
    "llm_name=\"gpt-4-1106-preview\"\n",
    "llm = OpenAI(model=llm_name, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801062cb",
   "metadata": {},
   "source": [
    "#### Define automerging functions  \n",
    "This Python function, `build_automerging_index`, is designed to build (or load, if it already exists) an auto-merging index from a collection of documents. Here's a breakdown of what each part of the function does and how it works:  \n",
    "\n",
    "  \n",
    "##### 1. **Function Definition:**\n",
    "- `def build_automerging_index(documents, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"merging_index_georg\", chunk_sizes=None):`\n",
    "This line defines the function with parameters for the documents to index, a language model (`llm`), an embedding model, the directory to save or load the index from, and the sizes for chunking documents.\n",
    "\n",
    "##### 2. **Setting Default Chunk Sizes:**\n",
    "- `chunk_sizes = chunk_sizes or [2048, 512, 128]`\n",
    "This sets the default sizes for document chunks if none are provided. Chunking can help manage memory and processing by breaking down the document set into more manageable pieces.\n",
    "\n",
    "##### 3. **Node Parser:**\n",
    "- `node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)`\n",
    "This creates a node parser that organizes documents into a hierarchical structure, which is beneficial for indexing and retrieval tasks. The parser uses the specified chunk sizes.\n",
    "\n",
    "##### 4. **Nodes from Documents:**\n",
    "- `nodes = node_parser.get_nodes_from_documents(documents)`\n",
    "This retrieves the nodes from the documents. Nodes typically represent chunks or pieces of the documents structured for efficient storage and retrieval.\n",
    "\n",
    "##### 5. **Leaf Nodes:**\n",
    "- `leaf_nodes = get_leaf_nodes(nodes)`\n",
    "This extracts the leaf nodes from the hierarchical structure. Leaf nodes often represent the smallest or most specific chunks of the documents, which are directly indexed and retrieved.\n",
    "\n",
    "##### 6. **Service Context:**\n",
    "- `merging_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)`\n",
    "This sets up a context for the indexing service, including the language model and embedding model. This context contains configurations and resources the indexing service needs.\n",
    "\n",
    "##### 7. **Storage Context:**\n",
    "- `storage_context = StorageContext.from_defaults()`\n",
    "This initializes a default storage context, which manages how and where the index and associated data are stored.\n",
    "- `storage_context.docstore.add_documents(nodes)`\n",
    "This line adds the documents (as nodes) to the document store within the storage context, preparing them for indexing.\n",
    "\n",
    "##### 8. **Check and Build or Load Index:**\n",
    "- The `if not os.path.exists(save_dir)` block checks whether an index already exists in the specified directory (`save_dir`). If it doesn't, the index is built from the leaf nodes and then saved to that directory. If the index does exist, it's loaded from the storage instead.\n",
    "\n",
    "##### 9. **Index Creation and Persistence:**\n",
    "- `automerging_index = VectorStoreIndex(...)`\n",
    "This line either creates a new index or loads an existing one, depending on whether it already exists. The index is created with leaf nodes and the specified storage and service contexts.\n",
    "- `automerging_index.storage_context.persist(persist_dir=save_dir)`\n",
    "If a new index is created, this line saves it to the specified directory so it can be quickly loaded in the future.\n",
    "\n",
    "##### 10. **Return the Index:**\n",
    "- `return automerging_index`\n",
    "Finally, the constructed or loaded auto-merging index is returned.\n",
    "\n",
    "**Summary:**\n",
    "The `build_automerging_index` function is a utility for creating or loading an auto-merging index based on a set of documents. It handles chunking, node parsing, and the setup of service and storage contexts. It's designed to work with hierarchical data structures and leverages a language model and an embedding model for document processing and indexing. This function is  part of a larger system for document retrieval where efficient, scalable search over large text corpora is necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8949da64-bc7f-4929-b394-51d11732e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build the automerging index\n",
    "def build_automerging_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index_georg\",\n",
    "    chunk_sizes=None,\n",
    "):\n",
    "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "    merging_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    # Check if the automerging index exists.\n",
    "    # If it does not exist, build it\n",
    "    # If it exists, load the index\n",
    "    if not os.path.exists(save_dir):\n",
    "        automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes, storage_context=storage_context, service_context=merging_context\n",
    "        )\n",
    "        automerging_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=merging_context,\n",
    "        )\n",
    "    return automerging_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e768b41",
   "metadata": {},
   "source": [
    "The following cell defines a Python function `get_automerging_query_engine` that sets up an automerging query engine. The engine is used for information retrieval tasks, where it retrieves documents or other data that are most similar to a given query and then refines (reranks) those results. Here's what each part of the function does:\n",
    "\n",
    "##### **Function Definition:**\n",
    "- `def get_automerging_query_engine(automerging_index, similarity_top_k=12, rerank_top_n=6):`\n",
    "This line defines the function with parameters for the auto-merging index, the number of top similar items to retrieve (`similarity_top_k`), and the number of items to rerank (`rerank_top_n`).\n",
    "\n",
    "##### **Base Retriever:**\n",
    "- `base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)`\n",
    "This creates a basic retriever from the auto-merging index. The `similarity_top_k` parameter indicates that the retriever should fetch the top-k most similar items to a given query. The base retriever is responsible for the initial retrieval of documents based on similarity.\n",
    "\n",
    "##### **AutoMerging Retriever:**\n",
    "- `retriever = AutoMergingRetriever(base_retriever, automerging_index.storage_context, verbose=True)`\n",
    "This line initializes an `AutoMergingRetriever`, likely a more advanced or specialized version of the basic retriever. It's given the `base_retriever` and the storage context from the auto-merging index. The `verbose=True` parameter indicates that the retriever should provide additional output or logging, useful for understanding its behavior and debugging.\n",
    "\n",
    "##### **Reranking with Sentence Transformers:**\n",
    "- `rerank = SentenceTransformerRerank(top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\")`\n",
    "This sets up a reranker using a sentence transformer model. The `rerank` object is configured to refine the top `rerank_top_n` results from the initial retrieval. The model specified, `\"BAAI/bge-reranker-base\"`, is likely a pre-trained transformer model designed for reranking or similar tasks, providing more nuanced understanding and ordering of the retrieved items.\n",
    "\n",
    "##### **Automerging Query Engine:**\n",
    "- `auto_merging_engine = RetrieverQueryEngine.from_args(retriever, node_postprocessors=[rerank])`\n",
    "This creates an automerging query engine, which manages the overall retrieval process. It's configured to use the `retriever` for initial retrieval and the `rerank` object as a postprocessor to refine the results. The engine encapsulates the process of taking a query, fetching relevant items, and applying post-processing to deliver a final set of refined results.\n",
    "\n",
    "##### **Return the Query Engine:**\n",
    "- `return auto_merging_engine`\n",
    "Finally, the function returns the fully configured automerging query engine.\n",
    "\n",
    "**Summary:**\n",
    "The `get_automerging_query_engine` function is a utility for setting up a query engine that first retrieves a set of documents or data based on similarity to a query and then reranks those results for improved relevance and accuracy. It uses an auto-merging index for initial retrieval and a sentence transformer model for reranking. This setup is typical in systems where efficient, scalable, and accurate search over large text corpora or similar datasets is necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e90d156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to set up the automerging query engine\n",
    "def get_automerging_query_engine(\n",
    "    automerging_index,\n",
    "    similarity_top_k=12,\n",
    "    rerank_top_n=6,\n",
    "):\n",
    "    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    retriever = AutoMergingRetriever(\n",
    "        base_retriever, automerging_index.storage_context, verbose=True\n",
    "    )\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "    auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever, node_postprocessors=[rerank]\n",
    "    )\n",
    "    return auto_merging_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb4ae9",
   "metadata": {},
   "source": [
    "#### Select the chunk size and build the automerging index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f52577-2e8e-4fee-ade9-2f5ce96a7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_merging_index = build_automerging_index(\n",
    "    documents,\n",
    "    llm=OpenAI(model=llm_name, temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index_georg\", # Give the name of the save directory. It can be different from the name in the function.\n",
    "    chunk_sizes=[2048,512], # Two splits\n",
    "    #chunk_sizes=[2048,512,128], # Three splits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f211b7",
   "metadata": {},
   "source": [
    "#### Set up the automerging query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c753885f-bfaf-4ac0-8410-b62f619827c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_merging_engine = get_automerging_query_engine(\n",
    "    auto_merging_index, # Pass the auto_merging_index created above\n",
    "    similarity_top_k=12,\n",
    "    rerank_top_n=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da722687",
   "metadata": {},
   "source": [
    "#### Ask a question on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f786d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "auto_merging_response = auto_merging_engine.query(\n",
    "    \"For how many days have all writs of execution been suspended?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f946c6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** All writs of execution have been suspended for a period of 180 days."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "display_response(auto_merging_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130e35d-bc09-4e4d-bbd9-61c9d6847110",
   "metadata": {},
   "source": [
    "### Evaluate a batch of questions with TruLens\n",
    "TruLens is a powerful open source library for evaluating and tracking large language model-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fea82",
   "metadata": {},
   "source": [
    "First set up the evaluation questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad216bb7-100e-455f-8c33-aa68b3e42026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For how many days have all writs of execution been suspended?', 'In what instances will bills of costs be suspended for 45 days?', 'List the full name of each respondent', 'How many people are injured in motor vehicle related accidents per annum?']\n"
     ]
    }
   ],
   "source": [
    "eval_questions = []\n",
    "with open('eval_questions_georg.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)\n",
    "        \n",
    "print(eval_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b7f942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For how many days have all writs of execution been suspended?', 'In what instances will bills of costs be suspended for 45 days?', 'List the full name of each respondent', 'How many people are injured in motor vehicle related accidents per annum?', 'Where shall this order be published by the applicant?']\n"
     ]
    }
   ],
   "source": [
    "# Optional: Here is how you can apend a new question:\n",
    "new_question = \"Where shall this order be published by the applicant?\"\n",
    "eval_questions.append(new_question)\n",
    "print(eval_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d81aa",
   "metadata": {},
   "source": [
    "#### Run the TruLens recorder  \n",
    " \n",
    "##### **Resetting the Database:**\n",
    "- `Tru().reset_database()`\n",
    "This line calls the `reset_database()` method on an instance of the `Tru` class. This method  resets or clears a database, removing all records or returning it to an initial state. This is useful in scenarios where you want to start fresh, perhaps during testing or when initializing a system.\n",
    "\n",
    "##### **Setting Up a Recorder:**\n",
    "- `tru_recorder = get_prebuilt_trulens_recorder(auto_merging_engine, app_id='app_0')`\n",
    "This line initializes a recorder, presumably for logging, monitoring, or analyzing queries and responses within the system. The specific function being used is `get_prebuilt_trulens_recorder`, a pre-configured recorder suited for use with the `auto_merging_engine`. The parameters provided are:  \n",
    "\n",
    "\n",
    "    - `auto_merging_engine`: This is the query engine responsible for retrieving and processing data. It's the same engine set up previously, which combines an automerging retriever with a reranking mechanism.\n",
    "    - `app_id='app_0'`: This specifies an application identifier, which is used to segregate or identify logs, configurations, or data associated with different applications or instances within a larger system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf034319-2f0a-43ac-896b-2aef1762b3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "Tru().reset_database()\n",
    "\n",
    "tru_recorder = get_prebuilt_trulens_recorder(\n",
    "    auto_merging_engine,\n",
    "    app_id ='app_0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9046c22",
   "metadata": {},
   "source": [
    "#### Define a function to run the evaluation questions on TruRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9426a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evals(eval_questions, tru_recorder, query_engine):\n",
    "    for question in eval_questions:\n",
    "        with tru_recorder as recording:\n",
    "            response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eea6f236-fe6a-470f-82b5-1a1e3d2593ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 2 nodes into parent node.\n",
      "> Parent node id: 5280a631-de3c-4d4f-ae42-9e39efda4f37.\n",
      "> Parent node text: JOHANNESBURG ON THIS 2197 DAY OF DECEMBER 2023.\n",
      "Guu\n",
      "MALATJI & CO ATTORNEYS\n",
      "Attorneys for the appl...\n",
      "\n",
      "> Merging 3 nodes into parent node.\n",
      "> Parent node id: fd7f78d0-3307-4090-ada4-a85b5661a7d9.\n",
      "> Parent node text: Respondent\n",
      "REG\n",
      "op HE\n",
      "AL\n",
      "Prvote Bga Z\n",
      "Potunis 000r\n",
      "2020 -2- 14 Sixteenth Respondent\n",
      "DF\n",
      "COPETONA\n",
      "PH...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: 261e66c0-1178-4222-b8be-d0f8600b23f0.\n",
      "> Parent node text: interest of justice.\n",
      "13.4 An appliçation to be admitted as an amicus curiae must briefly describe...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation function\n",
    "run_evals(eval_questions, tru_recorder, auto_merging_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea5627",
   "metadata": {},
   "source": [
    "#### Get the TruLens leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54cc6991-e743-4e33-9e22-414362a6aae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>app_0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.379167</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.004642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Answer Relevance  Context Relevance  Groundedness  latency  total_cost\n",
       "app_id                                                                        \n",
       "app_0                1.0           0.379167      0.730769     15.8    0.004642"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the TruLens leaderboard\n",
    "Tru().get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d1a31",
   "metadata": {},
   "source": [
    "#### Extract the records and feedback for each question in turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e663b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Answer Relevance_calls</th>\n",
       "      <th>Context Relevance_calls</th>\n",
       "      <th>Groundedness_calls</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>app_0</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_d261d63c7141b5d2963d4cf802769af0</td>\n",
       "      <td>\"For how many days have all writs of execution...</td>\n",
       "      <td>\"All writs of execution have been suspended fo...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_d261d63c7141b5d2963...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-31T01:30:51.910372\", \"...</td>\n",
       "      <td>2023-12-31T01:31:09.703046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[{'args': {'prompt': 'For how many days have a...</td>\n",
       "      <td>[{'args': {'prompt': 'For how many days have a...</td>\n",
       "      <td>[{'args': {'source': 'Page 7of184\n",
       "Ang\n",
       "21/22023...</td>\n",
       "      <td>17</td>\n",
       "      <td>2536</td>\n",
       "      <td>0.003812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>app_0</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_a5e6f67f3107ad26aa86351c224b1a7c</td>\n",
       "      <td>\"In what instances will bills of costs be susp...</td>\n",
       "      <td>\"Bills of costs will be suspended for 45 days ...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_a5e6f67f3107ad26aa8...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-31T01:31:10.464693\", \"...</td>\n",
       "      <td>2023-12-31T01:31:26.052551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[{'args': {'prompt': 'In what instances will b...</td>\n",
       "      <td>[{'args': {'prompt': 'In what instances will b...</td>\n",
       "      <td>[{'args': {'source': 'Page 7of184\n",
       "Ang\n",
       "21/22023...</td>\n",
       "      <td>15</td>\n",
       "      <td>2808</td>\n",
       "      <td>0.004230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>app_0</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_1a27303a16f51a0804cbecd5a3357a9d</td>\n",
       "      <td>\"List the full name of each respondent\"</td>\n",
       "      <td>\"AD Dandala &amp; Associates\\nGodla &amp; Partners\\nSi...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_1a27303a16f51a0804c...</td>\n",
       "      <td>{\"n_requests\": 2, \"n_successful_requests\": 2, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-31T01:31:26.667861\", \"...</td>\n",
       "      <td>2023-12-31T01:31:40.704344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>[{'args': {'prompt': 'List the full name of ea...</td>\n",
       "      <td>[{'args': {'prompt': 'List the full name of ea...</td>\n",
       "      <td>[{'args': {'source': 'The eleventh respondent,...</td>\n",
       "      <td>14</td>\n",
       "      <td>4762</td>\n",
       "      <td>0.007234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>app_0</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_96dc5f4ff9a40cd5a9ba9b07972bcc2f</td>\n",
       "      <td>\"How many people are injured in motor vehicle ...</td>\n",
       "      <td>\"The context information does not provide a sp...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_96dc5f4ff9a40cd5a9b...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-31T01:31:41.318718\", \"...</td>\n",
       "      <td>2023-12-31T01:31:56.680056</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[{'args': {'prompt': 'How many people are inju...</td>\n",
       "      <td>[{'args': {'prompt': 'How many people are inju...</td>\n",
       "      <td>[{'args': {'source': '1.13 Further and/or othe...</td>\n",
       "      <td>15</td>\n",
       "      <td>2758</td>\n",
       "      <td>0.004147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>app_0</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_3761c556159053d6b4944025d76f6e26</td>\n",
       "      <td>\"Where shall this order be published by the ap...</td>\n",
       "      <td>\"The order shall be published by the applicant...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_3761c556159053d6b49...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-31T01:31:57.371661\", \"...</td>\n",
       "      <td>2023-12-31T01:32:15.939319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>2495</td>\n",
       "      <td>0.003785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  app_id                                           app_json  \\\n",
       "0  app_0  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "1  app_0  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "2  app_0  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "3  app_0  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "4  app_0  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "\n",
       "                                                type  \\\n",
       "0  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "2  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "3  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "4  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "\n",
       "                                      record_id  \\\n",
       "0  record_hash_d261d63c7141b5d2963d4cf802769af0   \n",
       "1  record_hash_a5e6f67f3107ad26aa86351c224b1a7c   \n",
       "2  record_hash_1a27303a16f51a0804cbecd5a3357a9d   \n",
       "3  record_hash_96dc5f4ff9a40cd5a9ba9b07972bcc2f   \n",
       "4  record_hash_3761c556159053d6b4944025d76f6e26   \n",
       "\n",
       "                                               input  \\\n",
       "0  \"For how many days have all writs of execution...   \n",
       "1  \"In what instances will bills of costs be susp...   \n",
       "2            \"List the full name of each respondent\"   \n",
       "3  \"How many people are injured in motor vehicle ...   \n",
       "4  \"Where shall this order be published by the ap...   \n",
       "\n",
       "                                              output tags  \\\n",
       "0  \"All writs of execution have been suspended fo...    -   \n",
       "1  \"Bills of costs will be suspended for 45 days ...    -   \n",
       "2  \"AD Dandala & Associates\\nGodla & Partners\\nSi...    -   \n",
       "3  \"The context information does not provide a sp...    -   \n",
       "4  \"The order shall be published by the applicant...    -   \n",
       "\n",
       "                                         record_json  \\\n",
       "0  {\"record_id\": \"record_hash_d261d63c7141b5d2963...   \n",
       "1  {\"record_id\": \"record_hash_a5e6f67f3107ad26aa8...   \n",
       "2  {\"record_id\": \"record_hash_1a27303a16f51a0804c...   \n",
       "3  {\"record_id\": \"record_hash_96dc5f4ff9a40cd5a9b...   \n",
       "4  {\"record_id\": \"record_hash_3761c556159053d6b49...   \n",
       "\n",
       "                                           cost_json  \\\n",
       "0  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "1  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "2  {\"n_requests\": 2, \"n_successful_requests\": 2, ...   \n",
       "3  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "4  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "\n",
       "                                           perf_json  \\\n",
       "0  {\"start_time\": \"2023-12-31T01:30:51.910372\", \"...   \n",
       "1  {\"start_time\": \"2023-12-31T01:31:10.464693\", \"...   \n",
       "2  {\"start_time\": \"2023-12-31T01:31:26.667861\", \"...   \n",
       "3  {\"start_time\": \"2023-12-31T01:31:41.318718\", \"...   \n",
       "4  {\"start_time\": \"2023-12-31T01:31:57.371661\", \"...   \n",
       "\n",
       "                           ts  Answer Relevance  Context Relevance  \\\n",
       "0  2023-12-31T01:31:09.703046               1.0           0.366667   \n",
       "1  2023-12-31T01:31:26.052551               1.0           0.616667   \n",
       "2  2023-12-31T01:31:40.704344               1.0           0.533333   \n",
       "3  2023-12-31T01:31:56.680056               1.0           0.000000   \n",
       "4  2023-12-31T01:32:15.939319               NaN                NaN   \n",
       "\n",
       "   Groundedness                             Answer Relevance_calls  \\\n",
       "0      1.000000  [{'args': {'prompt': 'For how many days have a...   \n",
       "1      1.000000  [{'args': {'prompt': 'In what instances will b...   \n",
       "2      0.923077  [{'args': {'prompt': 'List the full name of ea...   \n",
       "3      0.000000  [{'args': {'prompt': 'How many people are inju...   \n",
       "4           NaN                                                NaN   \n",
       "\n",
       "                             Context Relevance_calls  \\\n",
       "0  [{'args': {'prompt': 'For how many days have a...   \n",
       "1  [{'args': {'prompt': 'In what instances will b...   \n",
       "2  [{'args': {'prompt': 'List the full name of ea...   \n",
       "3  [{'args': {'prompt': 'How many people are inju...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                  Groundedness_calls  latency  total_tokens  \\\n",
       "0  [{'args': {'source': 'Page 7of184\n",
       "Ang\n",
       "21/22023...       17          2536   \n",
       "1  [{'args': {'source': 'Page 7of184\n",
       "Ang\n",
       "21/22023...       15          2808   \n",
       "2  [{'args': {'source': 'The eleventh respondent,...       14          4762   \n",
       "3  [{'args': {'source': '1.13 Further and/or othe...       15          2758   \n",
       "4                                                NaN       18          2495   \n",
       "\n",
       "   total_cost  \n",
       "0    0.003812  \n",
       "1    0.004230  \n",
       "2    0.007234  \n",
       "3    0.004147  \n",
       "4    0.003785  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records, feedback = Tru().get_records_and_feedback(app_ids=[])\n",
    "records[:len(eval_questions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b9d7c",
   "metadata": {},
   "source": [
    "#### Print the questions, answers and the corresponding triad of metrics:  \n",
    "- Answer Relevance: is the answer relevant to the query? \n",
    "- Context Relevance: is the retrieved context relevant to the query? \n",
    "- Groundedness: is the answer relevant to the retrieved context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da70d39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"For how many days have all writs of execution been suspended?\"\n",
      "Answer: \"All writs of execution have been suspended for a period of 180 days.\"\n",
      "Answer Relevance: 1.00\n",
      "Context Relevance: 0.37\n",
      "Groundedness: 1.00\n",
      "\n",
      "\n",
      "Question: \"In what instances will bills of costs be suspended for 45 days?\"\n",
      "Answer: \"Bills of costs will be suspended for 45 days in cases where the bill of costs is not settled internally with the RAF but has to be taxed by the Taxing Master.\"\n",
      "Answer Relevance: 1.00\n",
      "Context Relevance: 0.62\n",
      "Groundedness: 1.00\n",
      "\n",
      "\n",
      "Question: \"List the full name of each respondent\"\n",
      "Answer: \"AD Dandala & Associates\\nGodla & Partners\\nSithombe Attorneys\\nKMalao Inc.\\nMduzulwana Attorneys and Legal/Consultants\\nDVDM Inc.\\nBe Broglio) Attorneys Inc.\\nVDS Attorneys\\nRoets & Van Rensburg\\nKorommbi Mabuli Inc.\\nSpruyt Inc.\\nPersonal Injury Plaintiffs Lawyers Association\\nAdvocate RAF Fee Recovery Association\"\n",
      "Answer Relevance: 1.00\n",
      "Context Relevance: 0.53\n",
      "Groundedness: 0.92\n",
      "\n",
      "\n",
      "Question: \"How many people are injured in motor vehicle related accidents per annum?\"\n",
      "Answer: \"The context information does not provide a specific number of people injured in motor vehicle related accidents per annum.\"\n",
      "Answer Relevance: 1.00\n",
      "Context Relevance: 0.00\n",
      "Groundedness: 0.00\n",
      "\n",
      "\n",
      "Question: \"Where shall this order be published by the applicant?\"\n",
      "Answer: \"The order shall be published by the applicant to all practicing attorneys through the Legal Practice Council, by email to all of the applicant's list of attorneys on its database, to the Minister of Transport by service on the State Attorney, and by publication in major newspapers in Gauteng, Western Cape, Northern Cape, Eastern Cape, Kwazulu-Natal, Free State, Limpopo, North-West, and Mpumalanga.\"\n",
      "Answer Relevance: nan\n",
      "Context Relevance: nan\n",
      "Groundedness: nan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through questions and outputs\n",
    "for idx, (question, output) in enumerate(zip(records[\"input\"][:len(eval_questions)], records[\"output\"][:len(eval_questions)])):\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Answer:\", output)\n",
    "    \n",
    "    # Print additional information if available, formatted to two decimal places\n",
    "    if \"Answer Relevance\" in records and len(records[\"Answer Relevance\"]) > idx:\n",
    "        print(f\"Answer Relevance: {records['Answer Relevance'][idx]:.2f}\")\n",
    "    if \"Context Relevance\" in records and len(records[\"Context Relevance\"]) > idx:\n",
    "        print(f\"Context Relevance: {records['Context Relevance'][idx]:.2f}\")\n",
    "    if \"Groundedness\" in records and len(records[\"Groundedness\"]) > idx:\n",
    "        print(f\"Groundedness: {records['Groundedness'][idx]:.2f}\")\n",
    "\n",
    "    print(\"\\n\")  # Print a newline for better readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7ef24",
   "metadata": {},
   "source": [
    "#### Finally, run the TruLens dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f204af1a-ab10-4e50-b2d6-ba78e2a66e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee56a543175f401385f6420e76fa6734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://localhost:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tru().run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d485ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007bb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
