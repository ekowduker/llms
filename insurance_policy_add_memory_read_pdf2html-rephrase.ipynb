{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa8f357-7268-4958-9540-e4126c012d37",
   "metadata": {},
   "source": [
    "### Extracting Information  \n",
    "This notebook extracts information from an insurance policy. \n",
    "This notebook uses:  \n",
    "- Langchain to load and retrieve information\n",
    "- Open AI language model to rephrase the user question and generate responses  \n",
    "- OpenAI Embeddings  \n",
    "- FAISS in-memory vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584eb08c-927d-40a0-a5a4-3c73e0a3ba54",
   "metadata": {},
   "source": [
    "##### Load the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca84868-e7b0-4547-b42a-8711b4494444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unstructured\n",
    "#!pip install tqdm # This useful for showing progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3625826-0e6f-4e0c-950b-66881a142a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.18\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa658af-c7f9-4fbc-bcb4-11c5cf208dcd",
   "metadata": {},
   "source": [
    "#### 1 - Import dependencies and set up the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599b6877-cadc-40ce-8e8e-cd467a991e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader, BSHTMLLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import LLMChain\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d66703-3db2-4d33-9f01-d7d0d1958a64",
   "metadata": {},
   "source": [
    "##### Read in the OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c91dd6-2fb1-42f3-bdca-e08ff74f9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Read OpenAI API key from environment variable\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"OpenAI API Key not set in environment variables\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123137f-a7e7-496b-a715-7272766ecfa2",
   "metadata": {},
   "source": [
    "##### Set up the language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4381bfda-bef9-4ecb-bacf-c5c4c3033901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the model used to rephrase the user question\n",
    "rephrase_model = ChatOpenAI(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    temperature = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6314ef67-da65-4032-808f-684e7e204bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the model used to generate responses\n",
    "model = ChatOpenAI(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    #model = \"gpt-4-1106-preview\", # Note: Map reduce with GPT4 is long and costly\n",
    "    temperature = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd57315-e094-4d67-912b-ea95058d3581",
   "metadata": {},
   "source": [
    "#### 3 - Create Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab07233-d472-4b7a-8293-e1997314b45d",
   "metadata": {},
   "source": [
    "##### Create a function to rephrase the user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0335dad-d142-4643-a7b5-14f7e60d850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_question(user_question):\n",
    "    rephrase_prompt = ChatPromptTemplate.from_template(f\"\"\"\n",
    "    You are an AI trained as an experienced insurance underwriter, well-versed in South African insurance policies. \n",
    "    Your responses should be careful, concise, authoritative, quantitative and factual, drawing specifically from the provided context. \n",
    "    If the answer to a question is not clearly supported by the context, state that you do not know the answer. \n",
    "    Avoid speculations or assumptions outside the given context. \n",
    "    Focus on delivering brief, factual, and directly relevant responses.\n",
    "    \n",
    "    Given the diverse range of inquiries an LLM receives, it's crucial to first present questions in a manner that enhances the \\\n",
    "    LLM's ability to comprehend and address them effectively. Your task is to refine the following user questions to make \\\n",
    "    them more specific, concise, and directly aligned with what an LLM can answer accurately.\\\n",
    "    Please rephrase each question below, aiming for clarity and directness in seeking information.\n",
    "    \n",
    "    The questions are not generic. They relate to a specific insurance policy document.\n",
    "    When answering a question, always give the corresponding amounts if you can.\n",
    "    Take your time and always think step by step.\n",
    "    \n",
    "    Example 1 (Understanding Coverage Limits):\n",
    "    Original: \"How much coverage do I have for personal property?\"\n",
    "    Rephrased: \"What are the specific coverage limits for personal property under this policy?\"\n",
    "    \n",
    "    Example 2 (Clarifying Exclusions):\n",
    "    Original: \"Is water damage from leaks covered?\"\n",
    "    Rephrased: \"Under what conditions does this policy cover water damage from plumbing leaks?\"\n",
    "    \n",
    "    Example 3 (Excess Details):\n",
    "    Original: \"Whatâ€™s my excess if something happens?\"\n",
    "    Rephrased: \"What are the excess amounts for different types of claims under this policy?\"\n",
    "\n",
    "    Example 4 (Cost breakdown):\n",
    "    Original: \"What makes up the total monthly premium for the Jaguar?\"\n",
    "    Rephrased: \"List all the quantified items that are included in the monthly premium for the jaguar.\"\n",
    "\n",
    "    Example 5 (Extensions)\n",
    "    Original: \"Tell me about the landslip extension.\"\n",
    "    Rephrased : \"Can you provide specific details about the landslip extension included in this policy?\"\n",
    "    \n",
    "    By adjusting the questions as demonstrated, you can better leverage the LLM's capabilities to provide informative and \\\n",
    "    precise answers. Now, proceed to rephrase the user question {user_question} following the examples provided.\n",
    "    If the user question {user_question} is phrased adequately, do not rephrase it. Simply return the user question.\n",
    "    \n",
    "    Rephrased question:\n",
    "    \"\"\")\n",
    "    \n",
    "    chain = LLMChain(llm=rephrase_model, prompt=rephrase_prompt)\n",
    "    answer = chain.invoke({\"user question\": user_question})\n",
    "    rephrased_question = answer['text'].replace('\"', '')\n",
    "\n",
    "    return rephrased_question\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad6bae6-7a9e-4fce-8dad-40bfa81ae644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses poppler, a PDF rendering library, commonly used as a backend for PDF to image, html or text conversion tools \n",
    "# in various programming environments.\n",
    "# You can ignore the warning: Syntax Warning: Bad annotation destination\n",
    "\n",
    "def convert_pdf_to_html(pdf_path, html_output_path):\n",
    "    # Construct the command to convert the PDF file to HTML\n",
    "    command = ['pdftohtml', '-c', '-noframes', pdf_path, html_output_path]\n",
    "\n",
    "    # Run the command\n",
    "    subprocess.run(command, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8046c-5a83-451d-a96a-8518bf96749b",
   "metadata": {},
   "source": [
    "##### Create a function to build the vector data base from the html document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6647b83a-fea8-496e-b195-34b347b1e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(source_html_file):\n",
    "    loader = UnstructuredHTMLLoader(source_html_file)\n",
    "    #loader = PyPDFLoader(\"./insurance/Southsure policy docs.pdf\")\n",
    "    pages=loader.load()\n",
    "\n",
    "   # Define chunk size, overlap and separators\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=8000, #experiment with this\n",
    "        chunk_overlap=800,\n",
    "        separators=['\\n\\n', '\\n', '(?=>\\. )', ' ', '']\n",
    "    )\n",
    "    docs  = text_splitter.split_documents(pages)\n",
    "\n",
    "    # Select the embeddings to use\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    #Create the vectorized db\n",
    "    # Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
    "\n",
    "    vector_db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    return vector_db, docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251c036-140d-458d-86e5-471f9a8ddb9f",
   "metadata": {},
   "source": [
    "##### Create a function to summarize the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f6525a0-bfa1-4729-9d4d-7fef3bde00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_doc(docs):\n",
    "\n",
    "    summarization_prompt = \"\"\"\n",
    "    Write a concise summary of the following:\n",
    "    {text}\n",
    "    CONCISE SUMMARY:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=summarization_prompt, input_variables=[\"text\"])\n",
    "\n",
    "    summary_chain = load_summarize_chain(\n",
    "    llm = model,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=prompt,\n",
    "    combine_prompt=prompt\n",
    "    )\n",
    "\n",
    "    summary = summary_chain.run(docs)\n",
    "    #print(summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7f9a4-e260-49d4-b036-2dcd58442bb9",
   "metadata": {},
   "source": [
    "##### Create a function to create the retrieval chain with conversation history and memory\n",
    "First add in Conversational History  \n",
    "Given a conversation history and a follow-up question, the goal is to rephrase the follow-up question so that it becomes a standalone question.  \n",
    "This means the rephrased question should be understandable without needing the context of the chat history.  \n",
    "\n",
    "Then format and combine multiple documents into a single string. Each document is formatted according to a specified or default template, and then they are combined into one string, with each document separated by a specified separator.  \n",
    "\n",
    "Finally, add in memory and return source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d69ba56c-1ef6-4b06-b89c-86cc14d0cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retrieval_chain():\n",
    "    # Given the chat history, rephrase the follow-up question to be a stand-alone question.\n",
    "    _template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question \\\n",
    "    to be a standalone question, in its original language.\n",
    "    \n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    Follow Up Input: {question}\n",
    "    Standalone question:\"\"\"\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "    # Create a summary prompt template from the document summary\n",
    "    summary_template = PromptTemplate(input_variables=[\"summary\"], template=\"{summary}\")\n",
    "\n",
    "    # Add it to the QA prompt template\n",
    "    template = summary_template.format(summary=summary) + \"\"\"\n",
    "    You are an AI trained as an experienced insurance underwriter, well-versed in South African insurance policies, \\\n",
    "    including interpreting tabulated data. Your responses should be quantitative, careful, concise, authoritative, and factual,\\\n",
    "    focusing specifically on extracting explicit numerical values provided in the context.\n",
    "    \n",
    "    When answering a question about the components of a monthly premium for a specific insured item, please ensure you list \\\n",
    "    not only the factors but also the explicit numerical values associated with each factor, \\\n",
    "    as stated in the policy document. For each factor contributing to the monthly premium, provide the exact amount or rate \\\n",
    "    if it is directly mentioned in the provided document.\n",
    "    \n",
    "    If the document explicitly lists figures, rates, or conditions, reference these details directly in your response. \\\n",
    "    If certain numerical values or specifics about the premium components are not mentioned or are unclear in the document, \\\n",
    "    state clearly that the numerical detail is not available. \\\n",
    "    Avoid speculation and focus on delivering factual responses with direct relevance to the question asked.\n",
    "    \n",
    "    Your role is to accurately convey the information as presented in the policy document, emphasizing the extraction and \\\n",
    "    reporting of numerical data to ensure clarity and precision in understanding the policy's cost breakdown.\n",
    "    \n",
    "    Only answer the question from the following context, with a special emphasis on directly quoted numerical data:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "\n",
    "    \"\"\"\n",
    "    ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # Format and combine multiple documents into a single string\n",
    "    DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "    def _combine_documents(\n",
    "        docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "    ):\n",
    "        doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "        return document_separator.join(doc_strings)\n",
    "\n",
    "    # Reset memory\n",
    "    memory = ConversationBufferMemory(\n",
    "        return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    "    )\n",
    "\n",
    "    # First we add a step to load memory\n",
    "    # This adds a \"memory\" key to the input object\n",
    "    loaded_memory = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    "    )\n",
    "    # Now we calculate the standalone question\n",
    "    standalone_question = {\n",
    "        \"standalone_question\": {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "        }\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | model\n",
    "        | StrOutputParser(),\n",
    "    }\n",
    "    # Now we retrieve the documents\n",
    "    retrieved_documents = {\n",
    "        \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "    # Now we construct the inputs for the final prompt\n",
    "    final_inputs = {\n",
    "        \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    # And finally, we do the part that returns the answers\n",
    "    answer = {\n",
    "        \"answer\": final_inputs | ANSWER_PROMPT | model,\n",
    "        \"docs\": itemgetter(\"docs\"),\n",
    "    }\n",
    "    # And now we put it all together!\n",
    "    final_chain = loaded_memory | standalone_question | retrieved_documents | answer\n",
    "\n",
    "    return final_chain, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72625c5b-98a7-4609-b9b1-75821ae60032",
   "metadata": {},
   "source": [
    "#### 4 - Run the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c4a2c-67ce-4b05-b765-4aa8dcefd713",
   "metadata": {},
   "source": [
    "##### A - Convert the original pdf to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "310eff14-dd33-4597-9bba-7825da25cbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page-1\n",
      "Page-2\n",
      "Page-3\n",
      "Page-4\n",
      "Page-5\n",
      "Page-6\n",
      "Page-7\n",
      "Page-8\n",
      "Page-9\n",
      "Page-10\n",
      "Page-11\n",
      "Page-12\n",
      "Page-13\n",
      "Page-14\n",
      "Page-15\n",
      "Page-16\n",
      "Page-17\n",
      "Page-18\n",
      "Page-19\n",
      "Page-20\n",
      "Page-21\n",
      "Page-22\n"
     ]
    }
   ],
   "source": [
    "pdf_file = 'insurance/Southsure policy docs.pdf' #This is the source pdf\n",
    "html_output_file = 'insurance/output_file' #This is the destination html file name\n",
    "\n",
    "convert_pdf_to_html(pdf_file, html_output_file)\n",
    "\n",
    "# This creates a file output_file.html as well as a png for each page.\n",
    "# The png files are required to maintain the html structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2c955-bf1f-4209-926c-fad5419e6983",
   "metadata": {},
   "source": [
    "##### B - Create the vector data base from the source OCR processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a859811-5da5-428b-8fb1-34e1653a03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db, docs = create_vector_db(source_html_file=\"./insurance/output_file.html\") \n",
    "#This is a one time event and can take some time depending on the length of the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea68d2-a8da-4944-964e-7104e974b650",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### C - Create a summary of the case or document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6f727-c51a-48e8-ae1e-64e3af88bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "current_time = datetime.now()\n",
    "# Format the date and time as a string\n",
    "formatted_time = current_time.strftime(\"Started summarizing document at %Y-%m-%d %H:%M:%S\")\n",
    "# Print the formatted date and time\n",
    "print(formatted_time)\n",
    "\n",
    "summary = summarize_doc(docs) #This is a one time event and can take some time\n",
    "\n",
    "# Get the current date and time\n",
    "current_time = datetime.now()\n",
    "# Format the date and time as a string\n",
    "formatted_time = current_time.strftime(\"Finished summarizing document at %Y-%m-%d %H:%M:%S\")\n",
    "# Print the formatted date and time\n",
    "print(formatted_time + \"\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f76b3-9cfb-4a76-a192-eea40bf527ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit as necesary\n",
    "summary = \"\"\"\n",
    "The document is an insurance policy for MISS NB RADEBE & MR EA DUKER\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b547fc-7078-4c49-b22e-4096c533de4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### D - Optional: Save summary to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200490f9-fb3d-410b-8d4f-92f7b7bca7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the filename where you want to save the summary\n",
    "filename = \"./insurance/summaries/summary.txt\"\n",
    "\n",
    "# Open the file in write mode and write the summary\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(summary)\n",
    "\n",
    "print(f\"The document summary has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab3d12-2251-474f-8f86-9acc4c69d09e",
   "metadata": {},
   "source": [
    "##### E - Optional: Load summary from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "462aa757-82c1-486f-a061-6b11c94ea7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " \n",
      "The document is an insurance policy for MISS NB RADEBE & MR EA DUKER\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the filename from which you want to load the summary\n",
    "filename = \"./insurance/summaries/summary.txt\"\n",
    "\n",
    "# Open the file in read mode and read the contents\n",
    "with open(filename, 'r') as file:\n",
    "    summary = file.read()\n",
    "\n",
    "# Optionally, print the loaded summary\n",
    "print(f\"Summary:\\n {summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e64a5-3d83-4b98-be3f-f902144fa221",
   "metadata": {},
   "source": [
    "#### 5 - Set up the retriever  \n",
    "    \n",
    "Parameters for search_type =  similarity_score_threshold:    \n",
    "search_type=\"similarity_score_threshold\",  \n",
    "search_kwargs={'score_threshold': 0.4},  \n",
    "  \n",
    "Parameters for search_type = Maximal Marginal Relevance (MMR)   \n",
    "k = number of documents to retrieve (default = 4)  \n",
    "lambda_mult = degree of diversity where 0 is more (default = 0.5)  \n",
    "fetch_k = number of documents to return to the mmr algorithm (default = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce888b85-4457-440d-b5cc-c47443f81633",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(\n",
    "    #search_type=\"similarity\",\n",
    "    search_type = 'similarity_score_threshold',\n",
    "    search_kwargs={'score_threshold': 0.5},\n",
    "    #search_type = 'mmr',\n",
    "    #search_kwargs={'k': 6, 'lambda_mult': 0.5, 'fetch_k': 2 },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7b744-3251-42b3-bfab-b99df50088f6",
   "metadata": {},
   "source": [
    "#### 6 - Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68b1840e-0e2f-4d04-9044-e13a066ad271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question here:  what makes up the monthly premium for the jaguar?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mYour question: what makes up the monthly premium for the jaguar?\u001b[0m\n",
      "\u001b[1mRephrased question: \u001b[0mWhat are the specific components included in the total monthly premium for the Jaguar?\n",
      "\u001b[1mAnswer: \u001b[0mThe specific components included in the total monthly premium for the Jaguar are as follows:\n",
      "\n",
      "1. Premium: 690.06\n",
      "2. Sasria Cover: 2.02\n",
      "3. Private and Professional Use: Included in the premium\n",
      "4. Comprehensive Cover Type: Included in the premium\n",
      "5. Tracking device installed: Included in the premium\n",
      "6. Basic Excess Waiver: Included in the premium\n",
      "7. Car Hire: Included in the premium\n",
      "8. Total: 692.08\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_question = input(\"Your question here: \")\n",
    "rephrased_question = rephrase_question(user_question)\n",
    "\n",
    "final_chain, memory = create_retrieval_chain()\n",
    "\n",
    "inputs = {\"question\": rephrased_question}\n",
    "result = final_chain.invoke(inputs)\n",
    "\n",
    "# Print results\n",
    "print(\"\\033[1m\" + \"Your question: \" + user_question + \"\\033[0m\")\n",
    "print(\"\\033[1m\" + \"Rephrased question: \" + \"\\033[0m\" +rephrased_question)\n",
    "print(\"\\033[1m\" + \"Answer: \" + \"\\033[0m\" + result['answer'].content)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1231a8-b3bd-42d4-b339-4185ed19f4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72d19ec2-59a5-4f34-a9a6-44821b8fc4b1",
   "metadata": {},
   "source": [
    "You can also ask questions without rephrasing to help optimise the examples in the rephrase_question function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "688b1f48-5de5-41c4-920e-a61a97917e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question here:  under whhat conditions would landslip cover be excluded?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mYour question: under whhat conditions would landslip cover be excluded?\u001b[0m\n",
      "The document does not explicitly state the conditions that would result in the exclusion of landslip cover.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask a question without rephrasing\n",
    "user_question = input(\"Your question here: \")\n",
    "final_chain, memory = create_retrieval_chain()\n",
    "\n",
    "inputs = {\"question\": user_question}\n",
    "result = final_chain.invoke(inputs)\n",
    "\n",
    "# Print results\n",
    "print(\"\\033[1m\" + \"Your question: \" + user_question + \"\\033[0m\")\n",
    "print(result['answer'].content)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4eeb9-6f9a-44a2-8f6a-ef4e2e49bbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c1369-b32a-472a-8928-ad3e8317b6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "750f85fd-2006-41d3-83b0-eb7260b1e81f",
   "metadata": {},
   "source": [
    "##### Alternate ways to summarize the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda4874-0aae-4a8a-b5c7-876b38c4dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "summary_map_reduce=chain.run(docs)\n",
    "print(summary_map_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074862e8-1256-4599-b880-49efaa1c7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "summary_refine=chain.run(docs)\n",
    "print(summary_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf7d0e-fcd7-4802-8fe3-35442eec9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For long documents, this method may exceed the model token limit\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "summary_stuff=chain.run(docs)\n",
    "print(summary_stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e1509-87c1-4536-a1e0-1f9cf7cbf549",
   "metadata": {},
   "source": [
    "References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0d254-0e35-4565-b467-356c725c6014",
   "metadata": {},
   "source": [
    "[Langchain: Retrieval Augmented Generation](https://python.langchain.com/docs/expression_language/cookbook/retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51ceaa-9b8a-4dd9-8286-cf69651ca0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
